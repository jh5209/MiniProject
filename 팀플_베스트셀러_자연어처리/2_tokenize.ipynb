{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ePvOMcRY3k2_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import re # 정규표현식\n",
        "import urllib.request\n",
        "from tqdm import tqdm # 작업 진행률 표시\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # 텍스트 토큰화\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # 길이가 다른 여러 문장의 길이를 임의로 동일하게 맞춰줌\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "okt = Okt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 클래스 선언\n",
        "class Tokenizer:\n",
        "\n",
        "    # 기본 생성자 선언\n",
        "    def __init__(self, path, path2):\n",
        "        self.path = path\n",
        "        self.path2 = path2\n",
        "\n",
        "        \n",
        "    # 멤버메서드 선언\n",
        "    def import_csv(self):\n",
        "        # 데이터프레임으로 불러오기\n",
        "        data = pd.read_csv(self.path, encoding='cp949')\n",
        "        # 한글과 공백 제외 모두 제거 : 띄어쓰기 유지, 구두점 제거\n",
        "        data['상품명'] = data['상품명'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]',\"\")\n",
        "   \n",
        "        # 불용어가 제거된 리스트 \n",
        "        bul_word = []\n",
        "        # 불용어 제거        \n",
        "        stopwords = ['의','가','이','은','들','을','하는','로','부터','처럼','만','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
        "        # 불용어 리스트 -> set\n",
        "        stop_words = set(stopwords)\n",
        "        for word_tokens in data['상품명']:\n",
        "            for word in okt.morphs(word_tokens):\n",
        "                if not word in stop_words:\n",
        "                    bul_word.append(word)\n",
        "        print(bul_word[:50])            \n",
        "        self.remove_dlist(bul_word) # 다른 멤버메소드에 변수를 엑세스할 때 사용\n",
        "    \n",
        "  \n",
        "    def remove_dlist(self, bul_word):\n",
        "\n",
        "        # 명사 추출\n",
        "        n_list = []\n",
        "        for word in bul_word:\n",
        "            nouns = okt.nouns(word)\n",
        "            n_list.append(nouns)\n",
        "            \n",
        "        # 2중 리스트를 faltten 하게 만들기\n",
        "        final_word = sum(n_list,[])\n",
        "            \n",
        "        # 빈도수 확인\n",
        "        count = Counter(final_word)\n",
        "        count_list = []\n",
        "        \n",
        "        for n, c in count.most_common(100):\n",
        "            dics = {'word':n, 'count':c}\n",
        "            count_list.append(dics)\n",
        "        \n",
        "        print(count_list[:5])\n",
        "\n",
        "        self.make_df(count_list)\n",
        "\n",
        "        \n",
        "    # 최종 데이터프레임 만들기    \n",
        "    def make_df(self, count_list):\n",
        "        # 리스트 안에 딕셔너리의 원하는 키값만 사용하여 리스트를 만든 후 데이터프레임으로 만들기\n",
        "        list1 = list(w['word'] for w in count_list)\n",
        "        list2 = list(c['count'] for c in count_list)\n",
        "        df_korean = pd.DataFrame(zip(list1, list2), columns=['word', 'count'])  \n",
        "        print(df_korean[:5])\n",
        "        self.export_csv(df_korean)\n",
        "        \n",
        "            \n",
        "    # csv 파일로 내보내기\n",
        "    def export_csv(self, df_korean):        \n",
        "        df_korean.to_csv(self.path2, index = False, encoding='cp949')\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\anaconda\\envs\\second_evm\\lib\\site-packages\\ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['당신', '꿈', '무엇', '입', '니까', '멈추지', '마', '다시', '꿈', '써', '봐', '나', '결심', '하', '지만', '뇌', '비웃는다', '하루', '분', '정리', '힘', '서른', '살', '엔', '미처', '몰랐던', '것', '내', '행복해지는', '거절', '힘', '사람', '무엇', '성장하는가', '시크릿', '미생', '리딩', '리드', '하', '라', '어떻게', '원하는', '것', '얻는가', '미생', '나', '남', '무엇', '다른', '영어']\n",
            "[{'word': '나', 'count': 560}, {'word': '법', 'count': 435}, {'word': '인생', 'count': 434}, {'word': '사람', 'count': 375}, {'word': '말', 'count': 369}]\n",
            "  word  count\n",
            "0    나    560\n",
            "1    법    435\n",
            "2   인생    434\n",
            "3   사람    375\n",
            "4    말    369\n"
          ]
        }
      ],
      "source": [
        "if __name__=='__main__':\n",
        "    path = 'D:\\\\VS_work\\\\MiniProject\\\\팀플_베스트셀러_자연어처리\\\\export_csv_file\\\\JG.csv'\n",
        "    path2 = 'D:\\\\VS_work\\\\MiniProject\\\\팀플_베스트셀러_자연어처리\\\\export_csv_file\\\\J_count.csv'\n",
        "    token = Tokenizer(path, path2)\n",
        "    token.import_csv()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "4jo-mecab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('second_evm')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "a1859a8c1bd11d1b02e51391c3d4f1c2366a7b3b6defad2afab35a0dc39ab796"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
